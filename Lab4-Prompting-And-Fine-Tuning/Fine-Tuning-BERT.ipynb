{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from transformers import EvalPrediction, pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T16:42:27.830496Z",
     "start_time": "2025-05-14T16:42:27.793911Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utils\n",
    "\n",
    "Helper classes and functions for our task"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6408b8bf767da190"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T16:42:28.334840Z",
     "start_time": "2025-05-14T16:42:28.329787Z"
    }
   },
   "id": "691a2799ac7f2fbc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class DisinformationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def load_and_process_data(file_path: str, label_column: str = \"label\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the data from a CSV file and processes the labels.\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        label_column (str): The column name containing the labels.\n",
    "        text_column (str): The column name containing the text content.\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe with labels and text content.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    data[label_column] = data[label_column].apply(lambda x: 1 if \"fake\" in x.lower() else 0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_metrics_to_json(metrics: dict, output_file_path: str):\n",
    "    \"\"\"\n",
    "    Saves the metrics to a JSON file.\n",
    "    Args:\n",
    "        metrics (dict): The evaluation metrics.\n",
    "        output_file_path (str): The file path to save the metrics.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        json.dump(metrics, output_file, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T16:42:28.680883Z",
     "start_time": "2025-05-14T16:42:28.676216Z"
    }
   },
   "id": "f9db223fbc190df0"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def compute_metrics(pred=None, y_true=None, y_pred=None):\n",
    "    \"\"\"\n",
    "    Computes F1 scores (micro, macro, weighted) for both training and testing data.\n",
    "\n",
    "    If `pred` is provided, it computes metrics for the trainer using `EvalPrediction`.\n",
    "    If `y_true` and `y_pred` are provided, it computes metrics for test data predictions.\n",
    "\n",
    "    Parameters:\n",
    "        - pred (EvalPrediction, optional): The evaluation prediction object for Trainer.\n",
    "        - y_true (list, optional): The ground truth labels for the test data.\n",
    "        - y_pred (list, optional): The predicted labels for the test data.\n",
    "\n",
    "    Returns:\n",
    "        - dict: A dictionary containing F1 metrics.\n",
    "    \"\"\"\n",
    "    if pred is not None:\n",
    "        # When working with the Trainer, pred is an EvalPrediction object\n",
    "        labels = pred.label_ids\n",
    "        y_pred = pred.predictions.argmax(-1)\n",
    "    elif y_true is not None and y_pred is not None:\n",
    "        # If y_true and y_pred are provided, use them for test evaluation\n",
    "        labels = y_true\n",
    "    else:\n",
    "        raise ValueError(\"Either `pred` or both `y_true` and `y_pred` must be provided.\")\n",
    "    \n",
    "        # Compute F1 scores\n",
    "    f1 = f1_score(y_true=labels, y_pred=y_pred)\n",
    "\n",
    "    return {\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "def compute_metrics_for_trainer(pred: EvalPrediction):\n",
    "    return compute_metrics(pred=pred)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T16:42:29.119744Z",
     "start_time": "2025-05-14T16:42:29.115593Z"
    }
   },
   "id": "786d74215708117"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fine Tuning BERT for Disinformation Detection\n",
    "\n",
    "Source of this lab session is [HuggingFace documentation and Learning Materials](https://huggingface.co/learn/llm-course/chapter3/1?fw=pt)\n",
    "\n",
    "HuggingFace Transformers library provides a Trainer class to help you fine-tune any of the pretrained models it provides on your dataset. You have just a few steps to do and define the Trainer. The hardest part is likely to be preparing the environment to run Trainer.train(), as it will run very slowly on a CPU. If you don’t have a GPU set up, you can get access to free GPUs or TPUs on Google Colab.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c21eef4d60fe66d"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Define the URLs pointing to the raw CSV data files hosted on GitHub.\n",
    "\n",
    "url_test = 'https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/ECTF/test.csv'\n",
    "url_train = 'https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/ECTF/train.csv'\n",
    "url_valid = 'https://raw.githubusercontent.com/ArkadiusDS/NLP-Labs/master/data/ECTF/validation.csv'\n",
    "\n",
    "# Download the datasets from GitHub using the wget command-line tool.\n",
    "# Each file is saved with a simple filename for ease of use.\n",
    "\n",
    "!wget -O test.csv {url_test}\n",
    "!wget -O train.csv {url_train}\n",
    "!wget -O validation.csv {url_valid}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T16:42:38.111044Z",
     "start_time": "2025-05-14T16:42:38.100363Z"
    }
   },
   "id": "80a1d86d9c5028ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load and preprocess the datasets using the custom function 'load_and_process_data'\n",
    "# This function will load the CSV data files, process the labels, and return the data in a usable dataframe format.\n",
    "\n",
    "# Load and process the training data\n",
    "train_data = load_and_process_data('train.csv')\n",
    "\n",
    "# Load and process the validation data\n",
    "validation_data = load_and_process_data('validation.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81b475083dd0a12c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is BERT?\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a powerful pre-trained language model developed by Google. It understands the context of words in a sentence by considering both the left and right context (bidirectional), making it effective for a wide variety of NLP tasks.\n",
    "\n",
    "Why load the model and tokenizer?\n",
    "\n",
    "Model: The AutoModelForSequenceClassification class loads a pre-trained version of BERT and enables further fine-tuning for classification tasks. For example, in our case, the model can predict whether a text is \"fake\" or \"real\".\n",
    "\n",
    "Tokenizer: The tokenizer converts raw text into tokens (smaller units of text) that the model can understand. It takes care of breaking down the text into the right format that matches the model's expectations (e.g., splitting the sentence into words or sub-words, and converting these into IDs).\n",
    "\n",
    "What does it mean \"uncased\"?\n",
    "The \"bert-base-uncased\" model is a version of BERT that doesn't distinguish between uppercase and lowercase letters. It treats \"Apple\" and \"apple\" the same, which can be helpful for some NLP tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12f17f959db1cfbb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id2label = {0: \"Credible\", 1: \"Fake\"}\n",
    "label2id = {\"Credible\": 0, \"Fake\": 1}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d425dad09e01856"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the pre-trained BERT model and tokenizer\n",
    "# BERT is a transformer-based model that has been pre-trained on a large corpus of text\n",
    "# We'll use it for classification task, where the model predicts labels for text.\n",
    "\n",
    "# Load the BERT model for classification (the base uncased version of BERT)\n",
    "# This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created with the from_pretrained()\n",
    "model = AutoModelForSequenceClassification.from_pretrained('google-bert/bert-base-uncased', num_labels=2, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Load the corresponding tokenizer for BERT\n",
    "# The tokenizer is responsible for converting the text into tokens that the model can process\n",
    "tokenizer = AutoTokenizer.from_pretrained('google-bert/bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a385ce2ca48128a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenize the datasets (training and validation) to prepare them for input into the BERT model.\n",
    "# Tokenization converts the raw text data into a format the BERT model can process.\n",
    "\n",
    "# Tokenizing the training dataset\n",
    "train_encodings = tokenizer(\n",
    "        train_data['content'].tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Tokenizing the validation dataset\n",
    "val_encodings = tokenizer(\n",
    "        validation_data['content'].tolist(),\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=256\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3182e1be28c74f54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Why do we use truncation?\n",
    "Text in a dataset can vary in length, and BERT-based models have a maximum input length they can handle (often 512 tokens). In this case, we're setting max_length=256, meaning that if a piece of text is longer than 256 tokens, it will be truncated (cut off) at that length. This ensures the text fits within BERT's expected input size.\n",
    "\n",
    "Why do we use padding?\n",
    "Some sentences or documents in the dataset may be shorter than 256 tokens. To ensure that all inputs to the model are the same length, we use padding. Padding adds extra tokens (usually with a value of 0) to the end of shorter sequences, so they all have the same number of tokens (256 in this case).\n",
    "\n",
    "What is max_length?\n",
    "The max_length=256 argument defines the maximum number of tokens for each input sequence. This ensures that all inputs are consistent in size, which is necessary for batch processing during model training or evaluation. If a sequence exceeds this length, it will be truncated; if it’s shorter, it will be padded."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b092c84ce69432f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create custom datasets for training and validation using the DisinformationDataset class.\n",
    "# These datasets will format the tokenized text data and corresponding labels into a format that can be used by the model during training and evaluation.\n",
    "\n",
    "# Create the training dataset: it combines the tokenized training data and corresponding labels\n",
    "train_dataset = DisinformationDataset(train_encodings, train_data['label'].tolist())\n",
    "\n",
    "# Create the validation dataset: it combines the tokenized validation data and corresponding labels\n",
    "val_dataset = DisinformationDataset(val_encodings, validation_data['label'].tolist())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be6bc38203764954"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output/training/',\n",
    "    eval_strategy='steps',\n",
    "    learning_rate=0.00001,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.06,\n",
    "    weight_decay=0.1,\n",
    "    fp16=True,\n",
    "    metric_for_best_model='f1',\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    greater_is_better=True,\n",
    "    save_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    save_on_each_node=True,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,  # Pass the actual model instance\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_for_trainer\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "235e27c4d1f6fc38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. What are TrainingArguments?\n",
    "\n",
    "- The TrainingArguments class is used to configure how the model should be trained. It controls aspects like learning rate, batch size, number of epochs, where to save the model, and how to handle evaluations and logging. You can think of it as the “training setup” that specifies how and where training will happen.\n",
    "\n",
    "2. Key training arguments:\n",
    "\n",
    "- output_dir: The output directory where the model predictions and checkpoints will be written.\n",
    "- eval_strategy: Specifies how often the model will be evaluated during training. In this case, evaluation happens at specific intervals (steps).\n",
    "- learning_rate: Controls how much the model adjusts its parameters with each update. A very small learning rate (0.00001) ensures more gradual changes to avoid overshooting optimal values. It is the initial learning rate for AdamW optimizer.\n",
    "- per_device_train_batch_size and per_device_eval_batch_size: These set the number of samples in each batch during training and evaluation.\n",
    "- num_train_epochs: Defines how many times the entire training dataset will be passed through the model. Typically, more epochs lead to better model performance, but too many can lead to overfitting.\n",
    "- warmup_ratio:  Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "- weight_decay: Regularization technique to prevent the model from overfitting.\n",
    "- fp16: Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training to speed up training on GPUs.\n",
    "- metric_for_best_model: Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\".\n",
    "- load_best_model_at_end: Ensures that the best model is loaded at the end of training for further use or evaluation.\n",
    "- save_strategy: The checkpoint save strategy to adopt during training.\n",
    "\n",
    "3. What is the Trainer?\n",
    "Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for HuggingFace Transformers.\n",
    "\n",
    "4. compute_metrics:\n",
    "This is a custom function you define that computes evaluation metrics like accuracy or F1 score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32d2c5ac004f0ea9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the model using the Trainer class.\n",
    "# This method will start the training process based on the configurations specified in the TrainingArguments.\n",
    "# The model will learn from the training data and be evaluated on the validation data according to the provided settings.\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a3c96367286933f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the trained model to a specified directory after training is completed.\n",
    "# This allows you to persist the model and use it for future predictions or fine-tuning without retraining.\n",
    "model_saved_path='output/final/'\n",
    "trainer.save_model(model_saved_path)\n",
    "tokenizer.save_pretrained(model_saved_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8a7af65717e9d24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prediction on Test Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "565ab91a704183fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the test data and preprocess\n",
    "test_data = load_and_process_data('test.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f2612e08782659b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the pipeline with CUDA\n",
    "classifier = pipeline(\n",
    "    task=\"text-classification\", \n",
    "    model=model_saved_path, \n",
    "    tokenizer=model_saved_path, \n",
    "    device=0,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "# Run pipeline on all content (batched)\n",
    "results = classifier(test_data[\"content\"].tolist(), batch_size=32)\n",
    "\n",
    "# Convert results to binary predictions\n",
    "test_data[\"predictions\"] = [1 if r[\"label\"] == \"Fake\" else 0 for r in results]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d49b455267a2f7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute evaluation metrics on the test data\n",
    "evaluation_results = compute_metrics(y_true=test_data[\"label\"], y_pred=test_data[\"predictions\"])\n",
    "\n",
    "# Save the evaluation metrics to a JSON file\n",
    "output_file_path = \"metrics/results.json\"\n",
    "save_metrics_to_json(evaluation_results, output_file_path)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ce58edf999cce1d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embeddings for Classical Machine Learning Models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb3c05f58a7c572d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T16:51:45.764725Z",
     "start_time": "2025-05-14T16:51:45.746394Z"
    }
   },
   "id": "d1ead19591645696"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Load and process the training data\n",
    "train_data = load_and_process_data('../data/ECTF/train.csv')\n",
    "\n",
    "# Load and process the validation data\n",
    "validation_data = load_and_process_data('../data/ECTF/validation.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-14T16:42:47.940742Z"
    }
   },
   "id": "bd4222bc3d1b930f"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                    id                                            content  \\\n0  1223697940247609346  Did Bill Gates &amp; World Economic Forum Pred...   \n1  1220941284896014336  RT @PascaleAllotey: Stay informed with updates...   \n2  1272293860425658368  RT @listenshahid: Prevention is the only cure ...   \n3  1234071862260457473  RT @YahooNews: TRUMP: “We are rapidly developi...   \n4  1259468230055407618  Get the latest updates about #COVID19 on our w...   \n\n   label  \n0      1  \n1      0  \n2      1  \n3      1  \n4      0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1223697940247609346</td>\n      <td>Did Bill Gates &amp;amp; World Economic Forum Pred...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1220941284896014336</td>\n      <td>RT @PascaleAllotey: Stay informed with updates...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1272293860425658368</td>\n      <td>RT @listenshahid: Prevention is the only cure ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1234071862260457473</td>\n      <td>RT @YahooNews: TRUMP: “We are rapidly developi...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1259468230055407618</td>\n      <td>Get the latest updates about #COVID19 on our w...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T16:43:42.622668Z",
     "start_time": "2025-05-14T16:43:42.605346Z"
    }
   },
   "id": "17a7d53731e38046"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T16:43:16.645790Z",
     "start_time": "2025-05-14T16:43:13.770765Z"
    }
   },
   "id": "9b015649a893320"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings_train = model.encode(train_data[\"content\"].to_list())\n",
    "embeddings_valid = model.encode(validation_data[\"content\"].to_list())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T17:23:49.582063Z",
     "start_time": "2025-05-14T17:22:57.216726Z"
    }
   },
   "id": "88b9eb887f1306b2"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3001, 768)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T17:23:49.584700Z",
     "start_time": "2025-05-14T17:23:49.581974Z"
    }
   },
   "id": "10683d5a4382e7d8"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(751, 768)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_valid.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T17:23:49.592287Z",
     "start_time": "2025-05-14T17:23:49.585628Z"
    }
   },
   "id": "ce73b9ffec480ffe"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(random_state=42)",
      "text/html": "<style>#sk-container-id-4 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-4 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-4 pre {\n  padding: 0;\n}\n\n#sk-container-id-4 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-4 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-4 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-4 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-4 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-4 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-4 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-4 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-4 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-4 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-4 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-4 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-4 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n#sk-container-id-4 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-4 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-4 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-4 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-4 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-4 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-4 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-4 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(random_state=42)</pre></div> </div></div></div></div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Train Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(embeddings_train, train_data[\"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T17:23:53.659675Z",
     "start_time": "2025-05-14T17:23:49.591780Z"
    }
   },
   "id": "c3df32d4e78dc70d"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.9036458333333334\n"
     ]
    }
   ],
   "source": [
    "# 6. Predict and evaluate\n",
    "y_pred = clf.predict(embeddings_valid)\n",
    "print(\"F1 score:\", f1_score(validation_data[\"label\"], y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T17:23:53.672772Z",
     "start_time": "2025-05-14T17:23:53.660233Z"
    }
   },
   "id": "e79a67e963a490cf"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8790560471976401\n"
     ]
    }
   ],
   "source": [
    "# Load and process the validation data\n",
    "test_data = load_and_process_data('../data/ECTF/test.csv')\n",
    "embeddings_test = model.encode(test_data[\"content\"].to_list())\n",
    "y_pred = clf.predict(embeddings_test)\n",
    "print(\"F1 score:\", f1_score(test_data[\"label\"], y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-14T17:24:01.377622Z",
     "start_time": "2025-05-14T17:23:53.682465Z"
    }
   },
   "id": "de249e0f37c37d77"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c348eeb01f8bdb1f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
