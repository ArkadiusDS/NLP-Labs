{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRdsJU_idw6c"
   },
   "outputs": [],
   "source": [
    "# !pip install -U datasets\n",
    "# !pip install transformers==4.38.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pK-VpshlcPU9"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtoY5MvX4MSZ"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VnpTZVv4MSa"
   },
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "\n",
    "    def __init__(self, developer_prompt, user_prompt_template):\n",
    "        self.developer_prompt = developer_prompt\n",
    "        self.user_prompt_template = user_prompt_template\n",
    "\n",
    "    def format_user_prompt(self, text):\n",
    "        return self.user_prompt_template.format(text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v16ds8VX4MSa"
   },
   "source": [
    "# Loading Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260,
     "referenced_widgets": [
      "fe5669202bb64c9c9de59ab5a28a1a2c",
      "dedc79b8462741a1a9465dfa2be4bcf4",
      "ace7a4d3b1e44a64a67b73454788ac52",
      "e3340d79e0db432cb095e33a7c184004",
      "94698f028e87454b93d7bc6288c4ed6d",
      "75ba984815ad4308adb91ad2c2aa2d44",
      "c4f13f007b704ce0970869676c49a63e",
      "22be232eeed44c0bb4283377a94f6dbb",
      "fb1175705be340afaaa46da9ca307bbe",
      "a3cabd7c52ce4723aa581ef9e61f2e68",
      "7df619264def45a2904accec4eed3311"
     ]
    },
    "id": "DHJ0JsiLayW-",
    "outputId": "1fb23d5a-c9a8-4a40-946b-96c533c6c748"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQU9NtsU4MSb"
   },
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mf6jHOa1a5V_",
    "outputId": "1cd35f58-736c-437b-c7e5-eec2a4adc480"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "print(f'Dataset length : {len(dataset[\"test\"])}')\n",
    "\n",
    "test_samples = dataset[\"test\"][:5]\n",
    "questions = test_samples[\"question\"]\n",
    "long_answers = test_samples[\"answer\"]\n",
    "answers = [float(an.split(\"#### \")[-1]) for an in long_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CIx0d1TeQcp"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"questions\":questions, \"long_answers\": long_answers, \"answer\": answers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SJ8RzDvv4MSb",
    "outputId": "56a94157-8c36-4c47-c853-6ca595ade76e"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kX_u3He4MSc"
   },
   "source": [
    "# Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggbXzPkLjLcm"
   },
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"You are a helpful AI assistant who knows math.\"\"\"\n",
    "user_prompt = \"\"\"Below I will provide a question with a math problem.\n",
    "Please solve it and present final number which is an answer to the problem.\n",
    "Do not show any explanation and do not provide units.\n",
    "\n",
    "Question: {text}\n",
    "Give answer in this form: {{\"answer\": \"answer with final number\"}}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(developer_prompt, user_prompt)\n",
    "developer_prompt = prompt_template.developer_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTPIIpMv4MSc"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t9vLBwgNg4kB",
    "outputId": "60ed43f6-2476-4081-ae0c-1226a9be6d30"
   },
   "outputs": [],
   "source": [
    "for index, question in tqdm(enumerate(df.questions.iloc[:5])):\n",
    "    user_prompt = prompt_template.format_user_prompt(question)\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    output = pipe(messages, **generation_args)\n",
    "    print(\"Raw output:\", output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JX1xF76O4MSc"
   },
   "source": [
    "# Zero Shot with Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOQxDg6DbkgK"
   },
   "outputs": [],
   "source": [
    "# Usage Example\n",
    "developer_prompt = \"You are a helpful AI assistant who knows math.\"\n",
    "user_prompt_template = \"\"\"Below I will provide a question with a math problem.\n",
    "Please solve it and present the final number which is the answer to the problem.\n",
    "In the final answer do not provide units, give only the number.\n",
    "\n",
    "Question: {text}\n",
    "Give answer in this form: {{\"reasoning\": \"Solve it step by step and provide reasoning and explanation\", \\n \"answer\": \"final number\"}}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(developer_prompt, user_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBYUMHfihGbW",
    "outputId": "fd8e709c-599a-4016-c38c-345c215ac302"
   },
   "outputs": [],
   "source": [
    "for index, question in tqdm(enumerate(df.questions.iloc[:5])):\n",
    "    user_prompt = prompt_template.format_user_prompt(question)\n",
    "    messages = [\n",
    "        {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    output = pipe(messages, **generation_args)\n",
    "\n",
    "    print(\"Raw output:\", output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEwl1Hb18nPo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
